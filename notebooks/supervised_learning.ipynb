{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f402aa3",
   "metadata": {},
   "source": [
    "# **Supervised Learning on Iris Dataset**\n",
    "\n",
    "In this assignment, we apply different supervised learning algorithms to classify the Iris dataset.\n",
    "\n",
    "Algorithms covered:\n",
    "- Q1: Naive Bayes\n",
    "- Q2: Decision Trees\n",
    "- Q3: k-Nearest Neighbors\n",
    "- Q4: Logistic Regression\n",
    "- Q5: Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ecd25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_friedman1, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom scripts\n",
    "# sys.path.append(\"../scripts\")\n",
    "# from naiveBayes import naiveBayes\n",
    "# import BasicTree\n",
    "\n",
    "# Sklearn classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e6b0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the naive Bayes function\n",
    "def naiveBayes(classes, learner, parameterised_function, train_data):\n",
    "    \"\"\"\n",
    "    Train a naive Bayes model by fitting a \n",
    "    parameterised distribution to each feature within each class, \n",
    "    assuming independence between features.\n",
    "\n",
    "    Args:\n",
    "        classes (list): List of possible class labels.\n",
    "        learner (function): Function that estimates parameters from training data \n",
    "                            (e.g., mean and std for Gaussian).\n",
    "        parameterised_function (function): Function that takes parameters and \n",
    "                                           returns a probability density function.\n",
    "        train_data (np.ndarray): Training data with features in columns and \n",
    "                                 class labels in the last column.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of functions, where each function g[class_value](test_data) \n",
    "              computes the unscaled likelihood for a batch of test points belonging to that class.\n",
    "    \"\"\"\n",
    "    f = {}           # feature-wise likelihood functions for each class\n",
    "    parameters = {}  # learned parameters per feature per class\n",
    "    g = {}           # class likelihood functions\n",
    "\n",
    "    for class_value in classes:\n",
    "        # Store parameters and functions for each feature of this class\n",
    "        parameters[class_value] = {}\n",
    "        f[class_value] = {}\n",
    "\n",
    "        # Extract only training samples belonging to the current class\n",
    "        train_x = train_data[train_data[:, -1] == class_value][:, :-1]\n",
    "\n",
    "        # Learn distribution parameters for each feature\n",
    "        for feature in range(train_x.shape[1]): \n",
    "            parameters[class_value][feature] = learner(train_x[:,feature])\n",
    "            f[class_value][feature] = parameterised_function(parameters[class_value][feature])\n",
    "\n",
    "        # Define a class-specific likelihood function\n",
    "        def create_g(class_value):     \n",
    "            def g(test_data):\n",
    "                # For each test point, compute likelihood of each feature independently\n",
    "                unscaled_feature_likelihoods = np.array([\n",
    "                    [f[class_value][feature](test_data[point, feature]) \n",
    "                     for feature in range(test_data.shape[1])]\n",
    "                    for point in range(test_data.shape[0])\n",
    "                ])\n",
    "                # Multiply feature likelihoods (independence assumption)\n",
    "                unscaled_point_likelihood = np.prod(unscaled_feature_likelihoods, axis=1).reshape(-1, 1)\n",
    "                return unscaled_point_likelihood\n",
    "            return g\n",
    "        \n",
    "        # Store the likelihood function for this class\n",
    "        g[class_value] = create_g(class_value)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "#  Example Usage \n",
    "# classes = [0,1]\n",
    "# def learner(train):\n",
    "#     mu = np.mean(train)\n",
    "#     sig = np.std(train)\n",
    "#     return [mu,sig]\n",
    "# def parameterised_function(parameters):\n",
    "#     mu, sig = parameters\n",
    "#     return lambda x: np.exp(-0.5*(x - mu)**2/(sig**2))\n",
    "# train_data = np.array([[2.0, 4.0, 0.0], [1.0, 5.0, 0.0],\n",
    "#                        [4.0, 2.0, 1.0], [6.0, 0.0, 1.0]])\n",
    "# g = naiveBayes(classes, learner, parameterised_function, train_data)\n",
    "# test_data = np.array([[2.0, 5.0], [3.0,3.0]])\n",
    "# for class_value in classes:\n",
    "#     print(g[class_value](test_data)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4c5ea",
   "metadata": {},
   "source": [
    "## **Question 1.1**\n",
    "### **What does the naive Bayes classifier actually return?**\n",
    "The naive Bayes classifier returns, for each class, a function that can evaluate the unscaled likelihood of a test point belonging to that class. In other words, it doesn’t directly give you probabilities, but it gives you a way to score new points by multiplying together feature-wise likelihoods under the independence assumption. The class with the highest score is then taken as the prediction.\n",
    "\n",
    "### **What do the functions do that are defined inside the main function?**\n",
    "The functions defined inside act as class-specific likelihood calculators. Each one takes in test data and applies the learnt feature distributions (from the training phase) to compute how likely the test point is if it were generated by that class. They are essentially wrappers that fix the class context and then process arbitrary test points.\n",
    "\n",
    "### **What is the role of the inputs to the naiveBayes function, in particular the learner and the parameterised function?**\n",
    "The `learner` is the procedure that estimates parameters from the training data for a single feature of a given class, for example, computing the mean and variance if we assume Gaussian features. The `parameterised_function` then takes those parameters and returns an actual density function that can score feature values. Together, these inputs define the modelling choice: `learner` extracts parameters, and `parameterised_function` turns them into usable feature-wise likelihood functions.\n",
    "\n",
    "## **Question 1.2: Where does the independence assumption made by the Naive Bayes approach come into the calculation?**\n",
    "The independence assumption appears when the classifier multiplies together the feature-wise likelihoods. Instead of modelling the full joint distribution of all features at once, it assumes each feature contributes independently. That’s why, in the code, the per-feature likelihood values are computed separately and then combined using a product across features.\n",
    "\n",
    "## **Question 1.3: What class of functions does the parameterised function in the example represent?**\n",
    "In the given example, the parameterised function represents Gaussian density functions. Each feature within a class is modelled as a normal distribution with its own mean and standard deviation. More generally, the parameterised function defines the family of distributions used to describe feature likelihoods (Gaussian in this case), but it could just as well be Bernoulli, multinomial, or any other parametric density depending on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4d722",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0142c7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column Overview\n",
      "+--------------+--------------+---------+-----------+\n",
      "|              | Column       | Dtype   |   Missing |\n",
      "|--------------+--------------+---------+-----------|\n",
      "| Sepal.Length | Sepal.Length | float64 |         0 |\n",
      "| Sepal.Width  | Sepal.Width  | float64 |         0 |\n",
      "| Petal.Length | Petal.Length | float64 |         0 |\n",
      "| Petal.Width  | Petal.Width  | float64 |         0 |\n",
      "| Species      | Species      | object  |         0 |\n",
      "+--------------+--------------+---------+-----------+\n",
      "\n",
      " Summary Statistics\n",
      "+----+---------+----------------+---------------+----------------+---------------+\n",
      "|    | index   |   Sepal.Length |   Sepal.Width |   Petal.Length |   Petal.Width |\n",
      "|----+---------+----------------+---------------+----------------+---------------|\n",
      "|  0 | count   |     150        |    150        |       150      |    150        |\n",
      "|  1 | mean    |       5.84333  |      3.05733  |         3.758  |      1.19933  |\n",
      "|  2 | std     |       0.828066 |      0.435866 |         1.7653 |      0.762238 |\n",
      "|  3 | min     |       4.3      |      2        |         1      |      0.1      |\n",
      "|  4 | 25%     |       5.1      |      2.8      |         1.6    |      0.3      |\n",
      "|  5 | 50%     |       5.8      |      3        |         4.35   |      1.3      |\n",
      "|  6 | 75%     |       6.4      |      3.3      |         5.1    |      1.8      |\n",
      "|  7 | max     |       7.9      |      4.4      |         6.9    |      2.5      |\n",
      "+----+---------+----------------+---------------+----------------+---------------+\n",
      "\n",
      " Species Value Counts\n",
      "+----+------------+---------+\n",
      "|    | Count      |   count |\n",
      "|----+------------+---------|\n",
      "|  0 | setosa     |      50 |\n",
      "|  1 | versicolor |      50 |\n",
      "|  2 | virginica  |      50 |\n",
      "+----+------------+---------+\n",
      "\n",
      " First 5 Rows of the Dataset\n",
      "+----+----------------+---------------+----------------+---------------+-----------+\n",
      "|    |   Sepal.Length |   Sepal.Width |   Petal.Length |   Petal.Width | Species   |\n",
      "|----+----------------+---------------+----------------+---------------+-----------|\n",
      "|  1 |            5.1 |           3.5 |            1.4 |           0.2 | setosa    |\n",
      "|  2 |            4.9 |           3   |            1.4 |           0.2 | setosa    |\n",
      "|  3 |            4.7 |           3.2 |            1.3 |           0.2 | setosa    |\n",
      "|  4 |            4.6 |           3.1 |            1.5 |           0.2 | setosa    |\n",
      "|  5 |            5   |           3.6 |            1.4 |           0.2 | setosa    |\n",
      "+----+----------------+---------------+----------------+---------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset with first column as index\n",
    "iris = pd.read_csv(\"../data/iris.csv\", index_col=0)\n",
    "\n",
    "# === Column headers, dtypes, missing values ===\n",
    "col_info = pd.DataFrame({\n",
    "    \"Column\": iris.columns,\n",
    "    \"Dtype\": iris.dtypes.astype(str),\n",
    "    \"Missing\": iris.isnull().sum().values\n",
    "})\n",
    "\n",
    "print(\" Column Overview\")\n",
    "print(tabulate(col_info, headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "# === Quick statistical summary ===\n",
    "print(\"\\n Summary Statistics\")\n",
    "print(tabulate(iris.describe().reset_index(), headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "# === Value counts for Species (categorical) ===\n",
    "print(\"\\n Species Value Counts\")\n",
    "print(tabulate(iris['Species'].value_counts().reset_index().rename(columns={'index': 'Species', 'Species': 'Count'}), headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "print(\"\\n First 5 Rows of the Dataset\")\n",
    "print(tabulate(iris.head(), headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0538c9",
   "metadata": {},
   "source": [
    "## **Question 1.4** \n",
    "We train a Naive Bayes classifier on the Iris dataset, using a train/test split to evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0bbaddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.9667\n",
      "\n",
      " Classification Report\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "|              |   precision |   recall |   f1-score |   support |\n",
      "|--------------+-------------+----------+------------+-----------|\n",
      "| setosa       |    1        | 1        |   1        | 10        |\n",
      "| versicolor   |    1        | 0.9      |   0.947368 | 10        |\n",
      "| virginica    |    0.909091 | 1        |   0.952381 | 10        |\n",
      "| accuracy     |    0.966667 | 0.966667 |   0.966667 |  0.966667 |\n",
      "| macro avg    |    0.969697 | 0.966667 |   0.966583 | 30        |\n",
      "| weighted avg |    0.969697 | 0.966667 |   0.966583 | 30        |\n",
      "+--------------+-------------+----------+------------+-----------+\n",
      "\n",
      " Confusion Matrix\n",
      "+------------+----------+--------------+-------------+\n",
      "|            |   setosa |   versicolor |   virginica |\n",
      "|------------+----------+--------------+-------------|\n",
      "| setosa     |       10 |            0 |           0 |\n",
      "| versicolor |        0 |            9 |           1 |\n",
      "| virginica  |        0 |            0 |          10 |\n",
      "+------------+----------+--------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "# Features (X) and target (y)\n",
    "X = iris.drop(columns=[\"Species\"])\n",
    "y = iris[\"Species\"]\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Gaussian Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy\")\n",
    "print(round(accuracy_score(y_test, y_pred), 4))\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n Classification Report\")\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "print(tabulate(pd.DataFrame(report).T, headers=\"keys\", tablefmt=\"psql\"))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n Confusion Matrix\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=nb.classes_)\n",
    "cm_df = pd.DataFrame(cm, index=nb.classes_, columns=nb.classes_)\n",
    "print(tabulate(cm_df, headers=\"keys\", tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0daebd",
   "metadata": {},
   "source": [
    "### Question 1.4 Observation\n",
    "The classifier performs very well, reaching an accuracy of about 96.7%. Setosa is perfectly separated, with no misclassifications at all. Versicolor has one sample misclassified as virginica, while virginica itself is predicted without errors. The precision and recall values are consistently high across all classes, showing that the model generalizes well to the test set. Overall, the naive Bayes approach is effective for this dataset, with only minor overlap between versicolor and virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a833897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tree: tree loss =  9.067077996170276\n",
      "DecisionTreeRegressor: tree loss =  10.197991295531748\n"
     ]
    }
   ],
   "source": [
    "# Define a function that generates synthetic regression data\n",
    "def makedata():\n",
    "    \"\"\"\n",
    "    Generate synthetic regression dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test split into training and testing sets.\n",
    "    \"\"\"\n",
    "    n_points = 500  # number of data points\n",
    " \n",
    "    X, y = make_friedman1(n_samples=n_points, n_features=5, \n",
    "                          noise=1.0, random_state=100)\n",
    "         \n",
    "    return train_test_split(X, y, test_size=0.5, random_state=3)\n",
    "\n",
    "# Define the main function to run the regression tree  \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate a manually implemented regression tree,\n",
    "    then compare with sklearn's DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = makedata()    \n",
    "    maxdepth = 10 # maximum tree depth             \n",
    "    \n",
    "    # Create tree root node\n",
    "    treeRoot = TNode(0, X_train, y_train) \n",
    "       \n",
    "    # Build the regression tree recursively\n",
    "    Construct_Subtree(treeRoot, maxdepth) \n",
    "    \n",
    "    # Predict on test set\n",
    "    y_hat = np.zeros(len(X_test))\n",
    "    for i in range(len(X_test)):\n",
    "        y_hat[i] = Predict(X_test[i], treeRoot)          \n",
    "    \n",
    "    MSE = np.mean(np.power(y_hat - y_test, 2))    \n",
    "    print(\"Basic tree: tree loss = \",  MSE)\n",
    "\n",
    "# Define the tree node class\n",
    "class TNode:\n",
    "    \"\"\"\n",
    "    Class representing a node in the regression tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, depth, X, y): \n",
    "        \"\"\"\n",
    "        Initialize tree node.\n",
    "\n",
    "        Args:\n",
    "            depth (int): Depth of the node in the tree.\n",
    "            X (np.ndarray): Matrix of explanatory variables.\n",
    "            y (np.ndarray): Vector of response values.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.j = None   # index of splitting variable\n",
    "        self.xi = None  # split threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.g = None   # regional predictor (mean of target values)\n",
    "      \n",
    "    def CalculateLoss(self):\n",
    "        \"\"\"\n",
    "        Compute sum of squared deviations from mean (impurity measure).\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value for this node.\n",
    "        \"\"\"\n",
    "        if len(self.y) == 0:\n",
    "            return 0\n",
    "        return np.sum(np.power(self.y - self.y.mean(), 2))\n",
    "                    \n",
    "# Define function to construct the regression sub-tree\n",
    "def Construct_Subtree(node, max_depth):  \n",
    "    \"\"\"\n",
    "    Recursively construct regression tree.\n",
    "\n",
    "    Args:\n",
    "        node (TNode): Current node.\n",
    "        max_depth (int): Maximum depth allowed.\n",
    "\n",
    "    Returns:\n",
    "        TNode: The constructed subtree rooted at this node.\n",
    "    \"\"\"\n",
    "    if node.depth == max_depth or len(node.y) == 1:\n",
    "        node.g = node.y.mean()  # Leaf node prediction\n",
    "    else:\n",
    "        j, xi = CalculateOptimalSplit(node)               \n",
    "        node.j = j\n",
    "        node.xi = xi\n",
    "        Xt, yt, Xf, yf = DataSplit(node.X, node.y, j, xi)\n",
    "              \n",
    "        if len(yt) > 0:\n",
    "            node.left = TNode(node.depth + 1, Xt, yt)\n",
    "            Construct_Subtree(node.left, max_depth)\n",
    "        \n",
    "        if len(yf) > 0:        \n",
    "            node.right = TNode(node.depth + 1, Xf, yf)\n",
    "            Construct_Subtree(node.right, max_depth)      \n",
    "     \n",
    "    return node\n",
    "\n",
    "# Define function to split dataset at a node\n",
    "def DataSplit(X, y, j, xi):\n",
    "    \"\"\"\n",
    "    Split dataset into left and right subsets at feature j and threshold xi.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target vector.\n",
    "        j (int): Feature index.\n",
    "        xi (float): Threshold value.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Xt, yt, Xf, yf) left and right splits.\n",
    "    \"\"\"\n",
    "    ids = X[:, j] <= xi      \n",
    "    Xt  = X[ids, :]\n",
    "    Xf  = X[~ids, :]\n",
    "    yt  = y[ids]\n",
    "    yf  = y[~ids]\n",
    "    return Xt, yt, Xf, yf             \n",
    "\n",
    "# Define function to calculate the optimal split at a node\n",
    "def CalculateOptimalSplit(node):\n",
    "    \"\"\"\n",
    "    Find the best split for a node by minimizing loss.\n",
    "\n",
    "    Args:\n",
    "        node (TNode): Node to split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_var, best_xi) best splitting feature index and threshold.\n",
    "    \"\"\"\n",
    "    X, y = node.X, node.y\n",
    "    best_var = 0\n",
    "    best_xi = X[0, best_var]          \n",
    "    best_split_val = node.CalculateLoss()\n",
    "    \n",
    "    m, n = X.shape\n",
    "    \n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            xi = X[i, j]\n",
    "            Xt, yt, Xf, yf = DataSplit(X, y, j, xi)\n",
    "            tmpt = TNode(0, Xt, yt) \n",
    "            tmpf = TNode(0, Xf, yf) \n",
    "            loss_t = tmpt.CalculateLoss()\n",
    "            loss_f = tmpf.CalculateLoss()    \n",
    "            curr_val = loss_t + loss_f\n",
    "            if curr_val < best_split_val:\n",
    "                best_split_val = curr_val\n",
    "                best_var = j\n",
    "                best_xi = xi\n",
    "    return best_var, best_xi\n",
    "\n",
    "# Define function to predict response for a single data point\n",
    "def Predict(X, node):\n",
    "    \"\"\"\n",
    "    Predict response for a single data point.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature vector.\n",
    "        node (TNode): Root of the decision tree.\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted value.\n",
    "    \"\"\"\n",
    "    if node.right is None and node.left is not None:\n",
    "        return Predict(X, node.left)\n",
    "    \n",
    "    if node.right is not None and node.left is None:\n",
    "        return Predict(X, node.right)\n",
    "    \n",
    "    if node.right is None and node.left is None:\n",
    "        return node.g\n",
    "    else:\n",
    "        if X[node.j] <= node.xi:\n",
    "            return Predict(X, node.left)\n",
    "        else:\n",
    "            return Predict(X, node.right)\n",
    "    \n",
    "# Run the main function\n",
    "main()  # run manual tree\n",
    "\n",
    "# Compare with sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "X_train, X_test, y_train, y_test = makedata()    \n",
    "regTree = DecisionTreeRegressor(max_depth=10, random_state=0)\n",
    "regTree.fit(X_train, y_train)\n",
    "y_hat = regTree.predict(X_test)\n",
    "MSE2 = np.mean(np.power(y_hat - y_test, 2))    \n",
    "print(\"DecisionTreeRegressor: tree loss = \",  MSE2)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad134e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f803e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f1fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed95e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1401d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d306dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282044e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea671d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15756a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
