{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f80f59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d84291",
   "metadata": {},
   "source": [
    "Part 1: Look through the code and understand how each of the functions work, and what each of their inputs and outputs mean. Take particular care to understand what happens with the function arguments that are called inside the exp_max function, and the dimensions of the matrices involved. Add appropriate comments to your copy of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_max(Iter, K, pdf, train, Xmat, W_Init, P_Init):\n",
    "    \"\"\"\n",
    "    Implements the Expectation-Maximization (EM) algorithm for clustering.\n",
    "\n",
    "    Args:\n",
    "        Iter (int): The number of iterations for which the algorithm should run.\n",
    "        K (int): The number of clusters.\n",
    "        pdf (function): A probability density function (e.g., normal_pdf) that takes parameters\n",
    "                        (e.g., mean) and data (Xmat) to return probabilities.\n",
    "        train (function): A learning function (e.g., normal_train) that fits the optimal parameters\n",
    "                          (e.g., mean) based on cluster probabilities and data.\n",
    "        Xmat (numpy.ndarray): An array storing the training data. Dimensions: (n_samples, n_features).\n",
    "        W_Init (numpy.ndarray): The initial weights for each cluster. Dimensions: (1, K).\n",
    "        P_Init (numpy.ndarray): The initial choice of parameters for each cluster (e.g., initial means).\n",
    "                                Dimensions: (n_features, K).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - W (numpy.ndarray): The final weights for each cluster. Dimensions: (1, K).\n",
    "            - P (numpy.ndarray): The final parameters for each cluster (e.g., final means).\n",
    "                                 Dimensions: (n_features, K).\n",
    "            - p (numpy.ndarray): The final probabilities associated with every cluster and data point.\n",
    "                                 Dimensions: (K, n_samples).\n",
    "    \"\"\"\n",
    "    n, D = Xmat.shape  # n = number of samples, D = number of features\n",
    "    p = np.zeros((K, n))  # Initialize probabilities: K clusters x n samples\n",
    "    W, P = W_Init, P_Init  # Initialize weights and parameters\n",
    "\n",
    "    for i in range(0, Iter):\n",
    "        # E-Step (Expectation Step): Calculate the probability of each data point belonging to each cluster\n",
    "        for k in range(0, K):\n",
    "            # p[k,:] calculates the likelihood of each data point Xmat under the current parameters P[:,k]\n",
    "            # and multiplies by the current weight W[0,k] for cluster k.\n",
    "            # pdf(P[:,k], Xmat) returns a 1D array of probabilities for each sample.\n",
    "            p[k,:] = W[0,k] * pdf(P[:,k], Xmat)\n",
    "\n",
    "        # Normalize probabilities so that for each data point, the sum of probabilities across all clusters is 1.\n",
    "        # p becomes the responsibility matrix (gamma in EM literature).\n",
    "        p = (p / np.sum(p, axis=0)) # Sum along axis 0 (columns) to normalize for each data point\n",
    "\n",
    "        # M-Step (Maximization Step): Update weights and parameters based on the calculated responsibilities\n",
    "        # Update weights (W): The new weight for each cluster is the average responsibility assigned to it.\n",
    "        W = np.mean(p, axis=1).reshape(1, K) # Mean along axis 1 (rows) to get average responsibility for each cluster\n",
    "\n",
    "        # Update parameters (P): For each cluster, train a new set of parameters using the weighted data points.\n",
    "        # The 'train' function takes the responsibilities for a cluster (p[k,:]) and the data (Xmat).\n",
    "        for k in range(0, K):\n",
    "            P[:,k] = train(p[k,:], Xmat)\n",
    "    return W, P, p\n",
    "\n",
    "def normal_train(p, Xmat):\n",
    "    \"\"\"\n",
    "    Trains the mean parameter for a Gaussian distribution.\n",
    "\n",
    "    Args:\n",
    "        p (numpy.ndarray): The responsibilities (probabilities) of data points belonging to a specific cluster.\n",
    "                           Dimensions: (n_samples,).\n",
    "        Xmat (numpy.ndarray): The training data. Dimensions: (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The calculated mean vector for the cluster. Dimensions: (n_features,).\n",
    "    \"\"\"\n",
    "    # Calculate the weighted mean of the data points based on their responsibilities.\n",
    "    # (Xmat.T @ p.T) performs a weighted sum of data points for each feature.\n",
    "    # sum(p) is the sum of responsibilities for the cluster.\n",
    "    m = (Xmat.T @ p.T) / np.sum(p)\n",
    "    return m\n",
    "\n",
    "def normal_pdf(m, Xmat, var=1):\n",
    "    \"\"\"\n",
    "    Calculates the probability density function (PDF) for a multivariate normal distribution.\n",
    "    Assumes a fixed, symmetric variance.\n",
    "\n",
    "    Args:\n",
    "        m (numpy.ndarray): The mean vector of the Gaussian distribution. Dimensions: (n_features,).\n",
    "        Xmat (numpy.ndarray): The data points for which to calculate the PDF. Dimensions: (n_samples, n_features).\n",
    "        var (float): The fixed variance for the symmetric Gaussian. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: An array of PDF values for each data point. Dimensions: (n_samples,).\n",
    "    \"\"\"\n",
    "    C = np.eye(Xmat.shape[1]) * var # Create a diagonal covariance matrix with 'var' on the diagonal\n",
    "    mvn = multivariate_normal(mean=m.T, cov=C)\n",
    "    return mvn.pdf(Xmat)\n",
    "\n",
    "\n",
    "# --- Q5: K-Means Algorithm (with comments) ---\n",
    "\n",
    "def kmeans(eps, K, Xmat, c_init):\n",
    "    \"\"\"\n",
    "    Implements the K-Means clustering algorithm.\n",
    "\n",
    "    Args:\n",
    "        eps (float): The convergence threshold. The algorithm stops when the change in centroids\n",
    "                     is less than this value.\n",
    "        K (int): The number of clusters.\n",
    "        Xmat (numpy.ndarray): The data points to be clustered. Dimensions: (n_samples, n_features).\n",
    "        c_init (numpy.ndarray): The initial centroids for each cluster. Dimensions: (n_features, K).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - c (numpy.ndarray): The final centroids for each cluster. Dimensions: (n_features, K).\n",
    "            - label (numpy.ndarray): An array indicating the cluster assignment for each data point.\n",
    "                                   Dimensions: (n_samples,).\n",
    "    \"\"\"\n",
    "    n, D = Xmat.shape  # n = number of samples, D = number of features\n",
    "    c = c_init.copy()  # Initialize centroids\n",
    "    c_old = np.zeros(c.shape)  # To store old centroids for convergence check\n",
    "    dist2 = np.zeros((K, n))  # To store squared distances from each point to each centroid\n",
    "\n",
    "    # Iterate until centroids converge\n",
    "    while np.abs(c - c_old).sum() > eps:\n",
    "        c_old = c.copy()\n",
    "\n",
    "        # E-Step (Assignment Step): Assign each data point to the nearest centroid\n",
    "        for i in range(0, K):  # Compute the squared distances from each point to centroid i\n",
    "            # (Xmat - c[:,i].T) calculates the difference between each data point and the current centroid i.\n",
    "            # **2 squares these differences.\n",
    "            # np.sum(..., 1) sums the squared differences across features to get the squared Euclidean distance.\n",
    "            dist2[i,:] = np.sum((Xmat - c[:,i].T)**2, 1)\n",
    "\n",
    "        # Assign each point to the cluster whose centroid is closest\n",
    "        label = np.argmin(dist2, 0)  # label[j] is the index of the closest centroid for data point j\n",
    "\n",
    "        # M-Step (Update Step): Recompute the centroids based on the new assignments\n",
    "        for i in range(0, K):  # For each cluster\n",
    "            entries = np.where(label == i)  # Get indices of data points assigned to cluster i\n",
    "            if len(entries[0]) > 0: # Ensure there are points in the cluster to avoid division by zero\n",
    "                # Recompute centroid i as the mean of all data points assigned to it\n",
    "                # entries[0] contains the indices, so we use Xmat[entries[0],:] to get the points\n",
    "                c[:,i] = np.mean(Xmat[entries[0],:], axis=0)\n",
    "            else:\n",
    "                # If a cluster becomes empty, keep its centroid in place or reinitialize (here, keeping in place)\n",
    "                pass\n",
    "    return c, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ac7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e2faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbef16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197a0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f37eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb69d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885e619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d622c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ed2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395ef53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73c70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48505301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98238a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db86459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cf34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
